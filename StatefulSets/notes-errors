https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/

A StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.
StatefulSet is the workload API object used to manage stateful applications.

Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.

Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.

If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.





Error 


Events:
  Type     Reason             Age   From                Message
  ----     ------             ----  ----                -------
  Warning  FailedScheduling   71s   default-scheduler   0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. not found
  Normal   NotTriggerScaleUp  72s   cluster-autoscaler  Pod didn't trigger scale-up:

errors

Events:
  Type     Reason             Age   From                Message
  ----     ------             ----  ----                -------
  Warning  FailedScheduling   12s   default-scheduler   0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. not found
  Normal   NotTriggerScaleUp  12s   cluster-autoscaler  Pod didn't trigger scale-up:



https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
Claude
################################################################################

Before my edit, the PV did **not** have `nodeAffinity`:

```yaml
# BEFORE - no nodeAffinity
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

```yaml
# AFTER - with nodeAffinity
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - kube-01
```

**Why it matters:**

| | Without nodeAffinity | With nodeAffinity |
|---|---|---|
| PV location | Any node | Locked to `kube-01` |
| Risk | PV created on wrong node, Pod can't access data | PV and Pod always on same node |
| `hostPath` data | May not be found | Guaranteed to match |

Since your Pod uses `hostPath` + `nodeSelector: kube-01`, the PV **must** also be pinned to `kube-01` â€” otherwise the Pod could theoretically bind to a PV on a different node where the `/mnt/data` path doesn't exist or has different data.
################################################################################
